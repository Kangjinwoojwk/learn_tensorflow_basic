{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1, 2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) # W,b 각각 -1.0~1.0 값의 균등분포 무작위값으로 초기화\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name=\"X\") # 자료입력 받을 플레이스 홀더 설정\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W*X+b # X,Y 상관관계를 분석하기 위한 수식 작성, W와의 곱, b와의 합으로 설명, W는 가중치 b는 편향\n",
    "# W,x가 행렬 아니므로 기본곱셈 연산자 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 경사하강법을 이용한 최적화 함수를 이용 손실값 최소 연산 그래프 생성\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.5811615 [1.0134743] [0.24150181]\n",
      "1 0.072186686 [0.9042976] [0.18781172]\n",
      "2 0.0061188857 [0.9184952] [0.18853034]\n",
      "3 0.0050799935 [0.91915417] [0.18342619]\n",
      "4 0.004829753 [0.9212398] [0.1790793]\n",
      "5 0.0046002367 [0.92311764] [0.17476752]\n",
      "6 0.004381715 [0.9249675] [0.17056698]\n",
      "7 0.004173584 [0.92677104] [0.16646656]\n",
      "8 0.003975341 [0.92853147] [0.16246486]\n",
      "9 0.003786498 [0.93024945] [0.1585593]\n",
      "10 0.0036066417 [0.93192625] [0.15474765]\n",
      "11 0.0034353184 [0.9335627] [0.15102762]\n",
      "12 0.0032721413 [0.9351598] [0.14739703]\n",
      "13 0.0031167099 [0.93671846] [0.1438537]\n",
      "14 0.0029686664 [0.93823975] [0.14039558]\n",
      "15 0.0028276534 [0.93972445] [0.13702057]\n",
      "16 0.0026933386 [0.9411734] [0.13372667]\n",
      "17 0.002565403 [0.94258755] [0.13051198]\n",
      "18 0.002443542 [0.94396776] [0.12737457]\n",
      "19 0.0023274736 [0.9453147] [0.12431256]\n",
      "20 0.0022169135 [0.9466293] [0.12132415]\n",
      "21 0.002111614 [0.94791234] [0.11840761]\n",
      "22 0.0020113029 [0.9491644] [0.11556114]\n",
      "23 0.0019157672 [0.95038646] [0.11278313]\n",
      "24 0.0018247683 [0.9515792] [0.11007193]\n",
      "25 0.0017380869 [0.9527432] [0.10742585]\n",
      "26 0.0016555245 [0.9538792] [0.1048434]\n",
      "27 0.0015768924 [0.9549879] [0.10232304]\n",
      "28 0.0015019908 [0.95607] [0.09986328]\n",
      "29 0.0014306377 [0.957126] [0.09746263]\n",
      "30 0.0013626887 [0.9581567] [0.0951197]\n",
      "31 0.0012979548 [0.95916253] [0.09283306]\n",
      "32 0.0012363043 [0.9601443] [0.09060143]\n",
      "33 0.0011775778 [0.96110237] [0.08842343]\n",
      "34 0.0011216414 [0.96203744] [0.0862978]\n",
      "35 0.0010683644 [0.9629501] [0.08422329]\n",
      "36 0.0010176167 [0.9638407] [0.0821986]\n",
      "37 0.00096927787 [0.96470994] [0.08022258]\n",
      "38 0.0009232349 [0.9655583] [0.07829411]\n",
      "39 0.0008793841 [0.96638626] [0.07641198]\n",
      "40 0.0008376138 [0.9671943] [0.07457507]\n",
      "41 0.0007978219 [0.9679829] [0.07278232]\n",
      "42 0.00075992354 [0.96875256] [0.0710327]\n",
      "43 0.000723829 [0.96950376] [0.06932513]\n",
      "44 0.00068944687 [0.97023684] [0.0676586]\n",
      "45 0.0006566958 [0.97095233] [0.06603214]\n",
      "46 0.0006255018 [0.9716506] [0.06444478]\n",
      "47 0.0005957925 [0.9723321] [0.06289558]\n",
      "48 0.0005674897 [0.97299725] [0.06138362]\n",
      "49 0.0005405345 [0.97364634] [0.05990798]\n",
      "50 0.00051486143 [0.97427994] [0.05846785]\n",
      "51 0.00049040094 [0.9748982] [0.05706231]\n",
      "52 0.00046710865 [0.9755016] [0.05569056]\n",
      "53 0.00044492146 [0.97609055] [0.0543518]\n",
      "54 0.0004237854 [0.9766653] [0.05304521]\n",
      "55 0.0004036548 [0.97722626] [0.05177004]\n",
      "56 0.00038448386 [0.9777737] [0.05052553]\n",
      "57 0.000366219 [0.978308] [0.04931094]\n",
      "58 0.0003488245 [0.97882944] [0.04812554]\n",
      "59 0.00033225506 [0.9793384] [0.04696864]\n",
      "60 0.00031647275 [0.9798351] [0.04583954]\n",
      "61 0.00030143745 [0.98031986] [0.0447376]\n",
      "62 0.00028712174 [0.980793] [0.04366215]\n",
      "63 0.00027348343 [0.9812547] [0.04261253]\n",
      "64 0.00026049084 [0.98170525] [0.04158813]\n",
      "65 0.00024811842 [0.9821451] [0.0405884]\n",
      "66 0.0002363323 [0.98257434] [0.0396127]\n",
      "67 0.00022510637 [0.9829932] [0.03866042]\n",
      "68 0.0002144131 [0.983402] [0.03773105]\n",
      "69 0.00020422965 [0.98380107] [0.03682405]\n",
      "70 0.00019452914 [0.98419046] [0.03593881]\n",
      "71 0.00018528877 [0.9845705] [0.03507487]\n",
      "72 0.00017648563 [0.9849414] [0.03423169]\n",
      "73 0.00016810454 [0.9853034] [0.03340878]\n",
      "74 0.00016011756 [0.9856567] [0.03260565]\n",
      "75 0.00015251397 [0.9860015] [0.03182184]\n",
      "76 0.00014526864 [0.9863381] [0.03105689]\n",
      "77 0.00013836903 [0.98666644] [0.03031028]\n",
      "78 0.00013179463 [0.98698694] [0.02958163]\n",
      "79 0.00012553479 [0.9872998] [0.02887053]\n",
      "80 0.00011957295 [0.9876051] [0.0281765]\n",
      "81 0.00011389243 [0.98790306] [0.02749915]\n",
      "82 0.00010848191 [0.98819387] [0.02683811]\n",
      "83 0.000103329636 [0.9884777] [0.02619295]\n",
      "84 9.8420736e-05 [0.9887547] [0.02556329]\n",
      "85 9.374679e-05 [0.989025] [0.02494876]\n",
      "86 8.929297e-05 [0.9892888] [0.02434901]\n",
      "87 8.505135e-05 [0.98954636] [0.02376369]\n",
      "88 8.1011734e-05 [0.9897976] [0.0231924]\n",
      "89 7.7163415e-05 [0.99004287] [0.02263488]\n",
      "90 7.34982e-05 [0.99028224] [0.02209076]\n",
      "91 7.00074e-05 [0.9905159] [0.02155972]\n",
      "92 6.668237e-05 [0.9907438] [0.02104141]\n",
      "93 6.351487e-05 [0.9909664] [0.0205356]\n",
      "94 6.0496615e-05 [0.99118346] [0.02004192]\n",
      "95 5.7623154e-05 [0.9913955] [0.01956016]\n",
      "96 5.4886874e-05 [0.9916023] [0.01908994]\n",
      "97 5.227963e-05 [0.9918042] [0.01863104]\n",
      "98 4.9795974e-05 [0.9920012] [0.01818316]\n",
      "99 4.742963e-05 [0.99219346] [0.01774604]\n",
      "X: 5, Y: [4.978713]\n",
      "X: 2.5, Y: [2.4982295]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y:y_data})\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X:5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X:2.5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

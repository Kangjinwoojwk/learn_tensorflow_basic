{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 제작, 앞 예제 같이 글자 학습, 원-핫 인코딩\n",
    "# 알파벳과 함글 나열 뒤 함글자씩 배열에 넣는다.\n",
    "# 넣는 글자는 연관 배열(키/값 쌍)dmfh qusrud\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "seq_data = [['word', '단어'], ['wood', '나무'], ['game', '놀이'], ['girl', '소녀'], ['kiss', '키스'],['love', '사랑']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 단어, 출력단어 한 글자씩 떼서 배열로 만든 후 원-핫 인코딩 유틸리티 함수 제작\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        # 인코더 셀의 입력값을 위해 입력 단어를 한글자씩 떼어 배열로 만든다.\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        # 디코더 셀의 입력값을 위해 출력 단어의 글자들을 배열로 만들고, 시작은 나타내는 심볼 'S'를 맨 앞에 붙입니다.\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # 학습을 위해 비교할 디코더 셀의 출력값을 만들고, 출력의 끝을 알려주는 심볼 'E'를 마지막에 붙입니다.\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 수치 정의\n",
    "learnging_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "n_class = n_input = dic_len\n",
    "# 인코더의 입력값, 디코더의 입력값, 출력값에 쓸 플레이스 홀더 구성\n",
    "# 입력은 [batch size, time steps, input size] 형식\n",
    "# 출력은 [batch size, time steps] 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성 다른 글자수의 것도 할 수 있게 일단 None, 단, 같은 배치때 입력되는 데이터는 글자수, 단계가 모두 같아야 한다.\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 15:07:33.009679 15436 deprecation.py:323] From <ipython-input-6-7e1ad10aaf9d>:3: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0808 15:07:33.026890 15436 deprecation.py:323] From <ipython-input-6-7e1ad10aaf9d>:6: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0808 15:07:33.120697 15436 deprecation.py:506] From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0808 15:07:33.137875 15436 deprecation.py:506] From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델을 위한 셀 구성\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "    \n",
    "# 디코더에 초기상대값아닌 인코더 최종값을 넣어야 한다.\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state = enc_states, dtype = tf.float32)\n",
    "# 인코더의 계산한 상태를 디코더로 전파 가능 initial_state = enc_states 옵션 쓰면 간단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0808 15:10:15.167797 15436 deprecation.py:323] From <ipython-input-7-281f6046d12f>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# 출력층 제작, 손실 함수와 최적화 함수 구성\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learnging_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.721476\n",
      "Epoch: 0002 cost = 2.690071\n",
      "Epoch: 0003 cost = 1.692258\n",
      "Epoch: 0004 cost = 1.471586\n",
      "Epoch: 0005 cost = 0.767640\n",
      "Epoch: 0006 cost = 0.542222\n",
      "Epoch: 0007 cost = 0.668714\n",
      "Epoch: 0008 cost = 0.242237\n",
      "Epoch: 0009 cost = 0.241188\n",
      "Epoch: 0010 cost = 0.103662\n",
      "Epoch: 0011 cost = 0.180770\n",
      "Epoch: 0012 cost = 0.332960\n",
      "Epoch: 0013 cost = 0.166649\n",
      "Epoch: 0014 cost = 0.041671\n",
      "Epoch: 0015 cost = 0.092945\n",
      "Epoch: 0016 cost = 0.085520\n",
      "Epoch: 0017 cost = 0.029800\n",
      "Epoch: 0018 cost = 0.019691\n",
      "Epoch: 0019 cost = 0.058325\n",
      "Epoch: 0020 cost = 0.125733\n",
      "Epoch: 0021 cost = 0.018935\n",
      "Epoch: 0022 cost = 0.012139\n",
      "Epoch: 0023 cost = 0.018184\n",
      "Epoch: 0024 cost = 0.033796\n",
      "Epoch: 0025 cost = 0.029963\n",
      "Epoch: 0026 cost = 0.021377\n",
      "Epoch: 0027 cost = 0.008317\n",
      "Epoch: 0028 cost = 0.005491\n",
      "Epoch: 0029 cost = 0.005918\n",
      "Epoch: 0030 cost = 0.004522\n",
      "Epoch: 0031 cost = 0.003950\n",
      "Epoch: 0032 cost = 0.005570\n",
      "Epoch: 0033 cost = 0.003715\n",
      "Epoch: 0034 cost = 0.005074\n",
      "Epoch: 0035 cost = 0.003497\n",
      "Epoch: 0036 cost = 0.001105\n",
      "Epoch: 0037 cost = 0.002617\n",
      "Epoch: 0038 cost = 0.003172\n",
      "Epoch: 0039 cost = 0.001041\n",
      "Epoch: 0040 cost = 0.002954\n",
      "Epoch: 0041 cost = 0.001170\n",
      "Epoch: 0042 cost = 0.001790\n",
      "Epoch: 0043 cost = 0.002169\n",
      "Epoch: 0044 cost = 0.001184\n",
      "Epoch: 0045 cost = 0.000928\n",
      "Epoch: 0046 cost = 0.000762\n",
      "Epoch: 0047 cost = 0.002755\n",
      "Epoch: 0048 cost = 0.001282\n",
      "Epoch: 0049 cost = 0.002953\n",
      "Epoch: 0050 cost = 0.000981\n",
      "Epoch: 0051 cost = 0.001696\n",
      "Epoch: 0052 cost = 0.001424\n",
      "Epoch: 0053 cost = 0.000508\n",
      "Epoch: 0054 cost = 0.001054\n",
      "Epoch: 0055 cost = 0.001700\n",
      "Epoch: 0056 cost = 0.001277\n",
      "Epoch: 0057 cost = 0.002836\n",
      "Epoch: 0058 cost = 0.001353\n",
      "Epoch: 0059 cost = 0.000407\n",
      "Epoch: 0060 cost = 0.002234\n",
      "Epoch: 0061 cost = 0.000909\n",
      "Epoch: 0062 cost = 0.001181\n",
      "Epoch: 0063 cost = 0.000720\n",
      "Epoch: 0064 cost = 0.000661\n",
      "Epoch: 0065 cost = 0.000563\n",
      "Epoch: 0066 cost = 0.001235\n",
      "Epoch: 0067 cost = 0.001783\n",
      "Epoch: 0068 cost = 0.001299\n",
      "Epoch: 0069 cost = 0.000784\n",
      "Epoch: 0070 cost = 0.000849\n",
      "Epoch: 0071 cost = 0.001129\n",
      "Epoch: 0072 cost = 0.001209\n",
      "Epoch: 0073 cost = 0.001868\n",
      "Epoch: 0074 cost = 0.000705\n",
      "Epoch: 0075 cost = 0.000495\n",
      "Epoch: 0076 cost = 0.000379\n",
      "Epoch: 0077 cost = 0.000629\n",
      "Epoch: 0078 cost = 0.000263\n",
      "Epoch: 0079 cost = 0.000419\n",
      "Epoch: 0080 cost = 0.000987\n",
      "Epoch: 0081 cost = 0.000854\n",
      "Epoch: 0082 cost = 0.001466\n",
      "Epoch: 0083 cost = 0.000431\n",
      "Epoch: 0084 cost = 0.000561\n",
      "Epoch: 0085 cost = 0.000852\n",
      "Epoch: 0086 cost = 0.001113\n",
      "Epoch: 0087 cost = 0.000803\n",
      "Epoch: 0088 cost = 0.000743\n",
      "Epoch: 0089 cost = 0.000635\n",
      "Epoch: 0090 cost = 0.000948\n",
      "Epoch: 0091 cost = 0.000419\n",
      "Epoch: 0092 cost = 0.000415\n",
      "Epoch: 0093 cost = 0.000860\n",
      "Epoch: 0094 cost = 0.000982\n",
      "Epoch: 0095 cost = 0.001032\n",
      "Epoch: 0096 cost = 0.002400\n",
      "Epoch: 0097 cost = 0.000633\n",
      "Epoch: 0098 cost = 0.001379\n",
      "Epoch: 0099 cost = 0.000353\n",
      "Epoch: 0100 cost = 0.000253\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "# 학습을 시키는 코드는 앞과 같아\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost], feed_dict = {enc_input: input_batch, dec_input:output_batch, targets:target_batch})\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    \n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 확인하기 위해 단어를 입력받아 번역 단어를 예측하는 함수 제작\n",
    "def translate(word):\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    \n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    # 예측 모델을 돌립니다. 세 번째 차원을 argmax로 취해 가장 확률이 높은 글자를 예측값으로 만듭니다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction, feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch})\n",
    "    \n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 번역 테스트 ===\n",
      "word -> 단어\n",
      "wodr -> 나무\n",
      "love -> 사랑\n",
      "loev -> 사랑\n",
      "abcd -> \n",
      "xcvf -> 사랑이랑\n"
     ]
    }
   ],
   "source": [
    "print('\\n=== 번역 테스트 ===')\n",
    "\n",
    "print('word ->', translate('word'))\n",
    "print('wodr ->', translate('wodr'))\n",
    "print('love ->', translate('love'))\n",
    "print('loev ->', translate('loev'))\n",
    "print('abcd ->', translate('abcd'))\n",
    "print('xcvf ->', translate('xcvf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
